/*
 * Copyright 2013, Pawe≈Ç Dziepak, pdziepak@quarnos.org.
 * Copyright 2023, Haiku, Inc. All Rights Reserved.
 * Distributed under the terms of the MIT License.
 */


#include "scheduler_cpu.h"

#include <cpu.h> // For cpu_ent, gCPU, irq_assignment, list_get_first_item etc.
#include <thread.h> // Explicitly include for thread_is_running
#include <util/AutoLock.h>
#include <util/atomic.h> // For atomic_add

#include <algorithm>
#include <stdlib.h> // For abs()

#include "scheduler_thread.h"
#include "EevdfRunQueue.h" // Make sure this is included


namespace Scheduler {


CPUEntry* gCPUEntries;

CoreEntry* gCoreEntries;
// CoreLoadHeap gCoreLoadHeap; // Replaced
// CoreLoadHeap gCoreHighLoadHeap; // Replaced
// rw_spinlock gCoreHeapsLock = B_RW_SPINLOCK_INITIALIZER; // Replaced
int32 gCoreCount;

CoreLoadHeap gCoreLoadHeapShards[kNumCoreLoadHeapShards];
CoreLoadHeap gCoreHighLoadHeapShards[kNumCoreLoadHeapShards];
rw_spinlock gCoreHeapsShardLock[kNumCoreLoadHeapShards]; // Initialized in scheduler_init

PackageEntry* gPackageEntries;
IdlePackageList gIdlePackageList;
rw_spinlock gIdlePackageLock = B_RW_SPINLOCK_INITIALIZER;
int32 gPackageCount;


}	// namespace Scheduler

using namespace Scheduler;


class Scheduler::DebugDumper {
public:
	static	void		DumpEevdfRunQueue(CPUEntry* cpu); // Renamed
	static	void		DumpCoreLoadHeapEntry(CoreEntry* core);
	static	void		DumpIdleCoresInPackage(PackageEntry* package);

private:
	// struct CoreThreadsData { // Unused
	// 		CoreEntry*	fCore; // Unused
	// 		int32		fLoad; // Unused
	// }; // Unused

	// static	void		_AnalyzeCoreThreads(Thread* thread, void* data); // Unused
};


static CPUPriorityHeap sDebugCPUHeap;
static CoreLoadHeap sDebugCoreHeap;


// Threshold for CoreEntry load changes to trigger global heap updates.
// kMaxLoad is 1000, so 50 is 5%.
static const int32 CORE_LOAD_UPDATE_DELTA_THRESHOLD = kMaxLoad / 20;


// ThreadRunQueue::Dump() is removed as ThreadRunQueue is removed.
// A new dump method for EevdfRunQueue will be part of DebugDumper.

static const int MAX_PEEK_ELIGIBLE_CANDIDATES = 3;


CPUEntry::CPUEntry()
	:
	// fMlfqHighestNonEmptyLevel is removed.
	fIdleThread(NULL), // Initialize fIdleThread
	fMinVirtualRuntime(0), // Initialize fMinVirtualRuntime. Will be updated by global_min_vruntime.
	fLoad(0),
	fInstantaneousLoad(0.0f),
	fInstLoadLastUpdateTimeSnapshot(0),
	fInstLoadLastActiveTimeSnapshot(0),
	fTotalThreadCount(0),
	fMeasureActiveTime(0),
	fMeasureTime(0),
	fUpdateLoadEvent(false)
{
	// fSchedulerModeLock was removed.
	B_INITIALIZE_SPINLOCK(&fQueueLock);
}


void
CPUEntry::Init(int32 id, CoreEntry* core)
{
	// Initializes this CPUEntry for the logical CPU specified by `id`,
	// associating it with its parent `core`.
	fCPUNumber = id;
	fCore = core;
	fIdleThread = NULL; // Set explicitly, though constructor does it.
	fMinVirtualRuntime = 0;
	// fMlfqHighestNonEmptyLevel removed.
	// Initialize load metrics to a clean state.
	fInstantaneousLoad = 0.0f;
	fInstLoadLastUpdateTimeSnapshot = system_time();
	fInstLoadLastActiveTimeSnapshot = gCPU[fCPUNumber].active_time;
	fTotalThreadCount = 0;

	// Initialize work-stealing fields
	fNextStealAttemptTime = 0;
	fLastTimeTaskStolenFrom = 0;
}


void
CPUEntry::Start()
{
	// Called when this CPU is being enabled for scheduling.
	// Resets load metrics and adds this CPU to its core's management.
	fLoad = 0;
	fInstantaneousLoad = 0.0f;
	fInstLoadLastUpdateTimeSnapshot = system_time();
	fInstLoadLastActiveTimeSnapshot = gCPU[fCPUNumber].active_time;
	fTotalThreadCount = 0;
	fMinVirtualRuntime = 0; // Reset on start as well
	fCore->AddCPU(this); // Register this CPU with its parent core.
}


void
CPUEntry::Stop()
{
	// Called when this CPU is being disabled for scheduling.
	// Migrates all IRQs off this CPU. Threads are migrated by higher-level logic
	// (e.g. by scheduler_set_cpu_enabled forcing its idle thread).
	cpu_ent* entry = &gCPU[fCPUNumber];

	SpinLocker locker(entry->irqs_lock);
	irq_assignment* irq
		= (irq_assignment*)list_get_first_item(&entry->irqs);
	while (irq != NULL) {
		locker.Unlock();
		assign_io_interrupt_to_cpu(irq->irq, -1);
		locker.Lock();
		irq = (irq_assignment*)list_get_first_item(&entry->irqs);
	}
	locker.Unlock();
}


void
CPUEntry::_UpdateMinVirtualRuntime()
{
	// Must be called with fQueueLock held.
	ASSERT(fQueueLock.IsOwnedByCurrentThread());

	bigtime_t oldLocalMinVR = fMinVirtualRuntime; // For tracing and as the "sticky" value
	bigtime_t localAnchorVR;

	ThreadData* head = fEevdfRunQueue.PeekMinimum();
	if (head != NULL && !head->IsIdle()) {
		localAnchorVR = head->VirtualRuntime();
	} else {
		// Queue is empty or only has idle thread. The anchor is the vruntime
		// of the last non-idle task that ran, which is already in fMinVirtualRuntime (oldLocalMinVR).
		localAnchorVR = oldLocalMinVR;
	}

	// These are defined in scheduler.cpp
	extern bigtime_t gGlobalMinVirtualRuntime;
	// extern spinlock gGlobalMinVRuntimeLock; // Lock no longer directly used by reader

	// Use atomic_get64 for lock-free read. util/atomic.h is included.
	bigtime_t currentGlobalMin = atomic_get64((int64*)&gGlobalMinVirtualRuntime);

	fMinVirtualRuntime = max_c(localAnchorVR, currentGlobalMin);

	TRACE_SCHED_CPU("CPUEntry %" B_PRId32 ": _UpdateMinVirtualRuntime: new fMinVR %" B_PRId64
		" (wasOldLocalMinVR %" B_PRId64 ", localAnchorVR %" B_PRId64 ", globalMin %" B_PRId64 ")\n",
		ID(), fMinVirtualRuntime, oldLocalMinVR, localAnchorVR, currentGlobalMin);

	// Proactively report this new local minimum to the global array.
	// gReportedCpuMinVR is declared in scheduler.cpp
	extern int64 gReportedCpuMinVR[MAX_CPUS]; // MAX_CPUS from smp.h
	atomic_set64(&gReportedCpuMinVR[fCPUNumber], fMinVirtualRuntime);
}


void
CPUEntry::AddThread(ThreadData* thread)
{
	SCHEDULER_ENTER_FUNCTION();
	ASSERT(are_interrupts_enabled() == false); // Check for holding spinlock
	ASSERT(thread != fIdleThread); // Idle thread should not be added to the main run queue.

	fEevdfRunQueue.Add(thread);
	thread->MarkEnqueued(this->Core()); // MarkEnqueued may need EEVDF context
	atomic_add(&fTotalThreadCount, 1);
	_UpdateMinVirtualRuntime(); // Update min_vruntime
	// fMlfqHighestNonEmptyLevel logic removed.
}


void
CPUEntry::RemoveThread(ThreadData* thread)
{
	SCHEDULER_ENTER_FUNCTION();
	ASSERT(thread->IsEnqueued() || thread == fIdleThread); // Idle thread might not be "enqueued" in fEevdfRunQueue
	ASSERT(are_interrupts_enabled() == false); // Check for holding spinlock

	if (thread != fIdleThread) {
		fEevdfRunQueue.Remove(thread);
		// Caller is responsible for threadData->MarkDequeued() if it's not the idle thread
		atomic_add(&fTotalThreadCount, -1);
		ASSERT(fTotalThreadCount >= 0);
		_UpdateMinVirtualRuntime(); // Update min_vruntime
	}
	// fMlfqHighestNonEmptyLevel logic removed.
}


// RemoveFromQueue is now obsolete as there's only one EEVDF queue per CPU.
// The caller should use RemoveThread directly.
/*
void
CPUEntry::RemoveFromQueue(ThreadData* thread, int mlfqLevel)
{
	...
}
*/


ThreadData*
CPUEntry::PeekEligibleNextThread()
{
	SCHEDULER_ENTER_FUNCTION();
	ASSERT(are_interrupts_enabled() == false); // Lock should be held by caller (reschedule)

	// Iterates through the EEVDF run queue (by popping and re-adding elements
	// not chosen) to find the first thread (by VirtualDeadline order) that is
	// currently eligible (system_time() >= EligibleTime()).
	// The chosen eligibleCandidate is *not* re-added to the queue.
	ThreadData* eligibleCandidate = NULL;
	// Use a DoublyLinkedList to temporarily store threads popped but not chosen.
	// This is because we don't know how many we might pop before finding an eligible one.
	DoublyLinkedList<ThreadData> temporarilyPoppedList;

	bigtime_t now = system_time();

	// Iterate through the heap by popping.
	// We are looking for the first eligible thread based on VirtualDeadline order.
	while (!fEevdfRunQueue.IsEmpty()) {
		ThreadData* candidate = fEevdfRunQueue.PopMinimum(); // Pop from actual queue
		if (candidate == NULL) // Should not happen if IsEmpty was false
			break;

		if (now >= candidate->EligibleTime()) {
			eligibleCandidate = candidate; // Found an eligible one
			// It's already popped, so we keep it out and will return it.
			// The caller (ChooseNextThread) will call MarkDequeued() on it.
			break;
		} else {
			// Not eligible, store it in our temporary list to re-add later.
			// We use a specific link within ThreadData for this temporary list if available,
			// or manage it externally if ThreadData doesn't have a generic extra link.
			// For simplicity, assuming ThreadData can be linked in a DoublyLinkedList directly
			// (it inherits from DoublyLinkedListLinkImpl).
			temporarilyPoppedList.Add(candidate);
			TRACE_SCHED("PeekEligibleNextThread: CPU %" B_PRId32 ", candidate thread %" B_PRId32 " (VD %" B_PRId64 ") not eligible (eligible_time %" B_PRId64 ", now %" B_PRId64 ").\n",
				ID(), candidate->GetThread()->id, candidate->VirtualDeadline(), candidate->EligibleTime(), now);
		}
	}

	// Re-add any threads that were popped from fEevdfRunQueue but found to be ineligible.
	// They are re-added in the order they were popped, though Add will place them
	// correctly by deadline.
	while (ThreadData* toReAdd = temporarilyPoppedList.RemoveHead()) {
		fEevdfRunQueue.Add(toReAdd);
		// fEevdfRunQueue.Add() calls _UpdateMinVirtualRuntime if the new item
		// becomes the head or if the queue was empty.
	}

	if (eligibleCandidate != NULL) {
		TRACE_SCHED("PeekEligibleNextThread: CPU %" B_PRId32 " chose eligible thread %" B_PRId32 " (VD %" B_PRId64 "). It has been removed from queue.\n",
			ID(), eligibleCandidate->GetThread()->id, eligibleCandidate->GetVirtualDeadline());
	} else {
		TRACE_SCHED("PeekEligibleNextThread: CPU %" B_PRId32 " no eligible thread found.\n", ID());
	}

	// If eligibleCandidate is NULL, all popped items were re-added.
	// If eligibleCandidate is non-NULL, it was removed.
	// _UpdateMinVirtualRuntime is called by PopMinimum if root changes or queue becomes empty,
	// and by Add if queue was empty or new item is root. This should cover most cases.
	// A final _UpdateMinVirtualRuntime is called in reschedule() after ChooseNextThread,
	// ensuring consistency.

	return eligibleCandidate;
}


// _UpdateHighestMLFQLevel is removed.


ThreadData*
CPUEntry::PeekIdleThread() const
{
	SCHEDULER_ENTER_FUNCTION();
	ASSERT(fIdleThread != NULL && fIdleThread->IsIdle());
	return fIdleThread;
}


void
CPUEntry::SetIdleThread(ThreadData* idleThread)
{
	SCHEDULER_ENTER_FUNCTION();
	ASSERT(idleThread != NULL && idleThread->IsIdle());
	fIdleThread = idleThread;
	// Typically, idle threads are not added to the main EEVDF run queue.
	// They are special and picked when no other eligible thread is available.
	// So, not modifying fTotalThreadCount or fEevdfRunQueue here.
	// Their `thread->scheduler_data->fCore` should be set correctly.
	if (fIdleThread->Core() == NULL) {
		// This might happen if idle threads are initialized very early.
		// Ensure it's associated with this CPU's core.
		// MarkEnqueued also sets fCore.
		fIdleThread->MarkEnqueued(this->Core());
		// Idle threads are conceptually always "ready" on their CPU, but not in the typical sense.
		// MarkDequeued might be called if it's treated like a normal thread being taken off a queue.
		// For simplicity, let's assume its state is managed.
	}
	ASSERT(fIdleThread->Core() == this->Core());
}


void
CPUEntry::UpdatePriority(int32 priority)
{
	SCHEDULER_ENTER_FUNCTION();
	ASSERT(!gCPU[fCPUNumber].disabled);

	int32 oldPriority = CPUPriorityHeap::GetKey(this);
	if (oldPriority == priority)
		return;
	fCore->CPUHeap()->ModifyKey(this, priority);

	if (oldPriority == B_IDLE_PRIORITY)
		fCore->CPUWakesUp(this);
	else if (priority == B_IDLE_PRIORITY)
		fCore->CPUGoesIdle(this);
}


void
CPUEntry::ComputeLoad()
{
	SCHEDULER_ENTER_FUNCTION();
	ASSERT(gTrackCPULoad);
	ASSERT(!gCPU[fCPUNumber].disabled);

	bigtime_t now = system_time();
	int oldLoad = compute_load(fMeasureTime, fMeasureActiveTime, fLoad, now);

	if (oldLoad < 0)
		return;

	if (fLoad > kVeryHighLoad && gCurrentMode != NULL)
		gCurrentMode->rebalance_irqs(false);

	UpdateInstantaneousLoad(now);
}

void
CPUEntry::UpdateInstantaneousLoad(bigtime_t now)
{
	SCHEDULER_ENTER_FUNCTION();
	if (!gTrackCPULoad || gCPU[fCPUNumber].disabled) {
		fInstantaneousLoad = 0.0f;
		fInstLoadLastUpdateTimeSnapshot = now;
		fInstLoadLastActiveTimeSnapshot = gCPU[fCPUNumber].active_time;
		if (fCore)
			fCore->UpdateInstantaneousLoad();
		return;
	}

	bigtime_t currentTotalActiveTime = gCPU[fCPUNumber].active_time;
	float currentActivitySample = 0.0f;

	if (fInstLoadLastUpdateTimeSnapshot == 0 || now <= fInstLoadLastUpdateTimeSnapshot) {
		// Handle potential system_time() wrap-around or first call carefully
		Thread* runningThread = gCPU[fCPUNumber].running_thread;
		if (runningThread != NULL && !thread_is_idle_thread(runningThread)) {
			currentActivitySample = 1.0f;
		} else {
			currentActivitySample = 0.0f;
		}
		// If it's the first call or time hasn't advanced, fInstantaneousLoad becomes currentActivitySample.
		// If time wrapped, this is a simple reset.
		fInstantaneousLoad = currentActivitySample;
	} else {
		bigtime_t timeDelta = now - fInstLoadLastUpdateTimeSnapshot;
		bigtime_t activeTimeDelta = currentTotalActiveTime - fInstLoadLastActiveTimeSnapshot;

		if (activeTimeDelta < 0) activeTimeDelta = 0; // active_time should not decrease
		if (activeTimeDelta > timeDelta) activeTimeDelta = timeDelta; // Cap at elapsed wall time

		currentActivitySample = (float)activeTimeDelta / timeDelta;
		// Clamping currentActivitySample to [0.0, 1.0] is redundant here,
		// as activeTimeDelta is already capped to be within [0, timeDelta]
		// and timeDelta is positive in this branch.

		fInstantaneousLoad = (kInstantLoadEWMAAlpha * currentActivitySample)
			+ ((1.0f - kInstantLoadEWMAAlpha) * fInstantaneousLoad);
	}

	fInstantaneousLoad = std::max(0.0f, std::min(1.0f, fInstantaneousLoad));

	fInstLoadLastActiveTimeSnapshot = currentTotalActiveTime;
	fInstLoadLastUpdateTimeSnapshot = now;

	if (fCore) {
		fCore->UpdateInstantaneousLoad();
	}
	// Update this CPU's score in its core's CPUHeap
	// Higher score is better (less loaded).
	UpdatePriority(kMaxLoad - (int32)(fInstantaneousLoad * kMaxLoad));
}


ThreadData*
CPUEntry::ChooseNextThread(ThreadData* oldThread, bool /*putAtBack*/, int /*oldMlfqLevel*/)
{
	SCHEDULER_ENTER_FUNCTION();
	ASSERT(are_interrupts_enabled() == false); // Caller must hold the run queue lock.

	// If the old thread (that was just running) is still ready and belongs to
	// this CPU's core, its EEVDF parameters should be updated, and it should be
	// re-inserted into the EEVDF run queue.
	if (oldThread != NULL && oldThread->GetThread()->state == B_THREAD_READY
		&& oldThread->Core() == this->Core() && oldThread != fIdleThread) {
		// The actual update of oldThread's EEVDF parameters (vruntime, lag, deadline)
		// happens in reschedule() before this function is called if oldThread is re-queued.
		// Here, we just add it back.
		fEevdfRunQueue.Add(oldThread);
		_UpdateMinVirtualRuntime(); // Update min_vruntime as queue changed
		// oldThread->MarkEnqueued() was called by AddThread.
	}

	// Peek the next eligible thread from the EEVDF run queue.
	ThreadData* nextThreadData = PeekEligibleNextThread(); // This is now non-const

	if (nextThreadData != NULL) {
		// nextThreadData is already removed from the queue by PeekEligibleNextThread
		// and needs to be marked as dequeued.
		nextThreadData->MarkDequeued();
	} else {
		// No eligible, non-idle thread found. Choose this CPU's idle thread.
		nextThreadData = fIdleThread;
		if (nextThreadData == NULL) { // Should have been set during init
			panic("CPUEntry::ChooseNextThread: CPU %" B_PRId32 " has no fIdleThread assigned!", ID());
		}
		// Idle thread is not "dequeued" from the EEVDF queue in the same way.
		// Its fEnqueued state should reflect its special status.
	}

	ASSERT(nextThreadData != NULL);
	return nextThreadData;
}


void
CPUEntry::TrackActivity(ThreadData* oldThreadData, ThreadData* nextThreadData)
{
	SCHEDULER_ENTER_FUNCTION();

	// This function is called after a context switch (or when the scheduler
	// decides the current thread will continue running). It updates various
	// time and load accounting metrics for the CPU, core, and the threads involved.

	cpu_ent* cpuEntry = &gCPU[fCPUNumber]; // Low-level per-CPU data.
	Thread* oldThread = oldThreadData->GetThread();

	// Account for the time the oldThread just spent running.
	if (!thread_is_idle_thread(oldThread)) {
		bigtime_t activeKernelTime = oldThread->kernel_time - cpuEntry->last_kernel_time;
		bigtime_t activeUserTime = oldThread->user_time - cpuEntry->last_user_time;
		bigtime_t activeTime = activeKernelTime + activeUserTime;

		if (activeTime < 0) activeTime = 0; // Sanity check.

		// Update overall CPU active time (used by cpufreq, etc.).
		WriteSequentialLocker locker(cpuEntry->active_time_lock);
		cpuEntry->active_time += activeTime;
		locker.Unlock();

		// Update this CPUEntry's measurement of active time for its fLoad calculation.
		fMeasureActiveTime += activeTime;
		// Update the parent CoreEntry's cumulative active time.
		if (fCore) fCore->IncreaseActiveTime(activeTime);

		// Let the thread itself account for its consumed CPU time.
		// This now also updates fTimeUsedInCurrentQuantum
		oldThreadData->UpdateActivity(activeTime);
	}

	// If CPU load tracking is enabled, update load metrics and potentially
	// request a CPU performance level change.
	if (gTrackCPULoad) {
		if (!cpuEntry->disabled)
			ComputeLoad(); // Updates fLoad and calls UpdateInstantaneousLoad.
		else // Ensure instantaneous load is zeroed if CPU is disabled.
			UpdateInstantaneousLoad(system_time());
		_RequestPerformanceLevel(nextThreadData); // Request cpufreq change based on new state.
	}

	// Prepare for the nextThread's run.
	Thread* nextThread = nextThreadData->GetThread();
	if (!thread_is_idle_thread(nextThread)) {
		// Store current kernel/user times to calculate usage at next reschedule.
		cpuEntry->last_kernel_time = nextThread->kernel_time;
		cpuEntry->last_user_time = nextThread->user_time;
		// Store interrupt time to account for stolen time later.
		nextThreadData->SetLastInterruptTime(gCPU[fCPUNumber].interrupt_time);
	}
}


void
CPUEntry::StartQuantumTimer(ThreadData* thread, bool wasPreempted, bigtime_t sliceDuration)
{
	cpu_ent* cpu = &gCPU[ID()];

	if (!wasPreempted || fUpdateLoadEvent)
		cancel_timer(&cpu->quantum_timer);
	fUpdateLoadEvent = false;

	if (!thread->IsIdle()) {
		add_timer(&cpu->quantum_timer, &CPUEntry::_RescheduleEvent, sliceDuration,
			B_ONE_SHOT_RELATIVE_TIMER);
	} else if (gTrackCoreLoad || gTrackCPULoad) {
		add_timer(&cpu->quantum_timer, &CPUEntry::_UpdateLoadEvent,
			kLoadMeasureInterval, B_ONE_SHOT_RELATIVE_TIMER);
		fUpdateLoadEvent = true;
	}
}


void
CPUEntry::_RequestPerformanceLevel(ThreadData* threadData)
{
	SCHEDULER_ENTER_FUNCTION();

	if (gCPU[fCPUNumber].disabled) {
		decrease_cpu_performance(kCPUPerformanceScaleMax);
		return;
	}

	int32 loadToConsider = static_cast<int32>(this->GetInstantaneousLoad() * kMaxLoad);

	ASSERT_PRINT(loadToConsider >= 0 && loadToConsider <= kMaxLoad, "load is out of range %"
		B_PRId32, loadToConsider);

	if (loadToConsider < kTargetLoad) {
		int32 delta = kTargetLoad - loadToConsider;
		delta = (delta * kCPUPerformanceScaleMax) / (kTargetLoad > 0 ? kTargetLoad : 1) ;
		decrease_cpu_performance(delta);
	} else {
		int32 range = kMaxLoad - kTargetLoad;
		if (range <=0) range = 1;
		int32 delta = loadToConsider - kTargetLoad;
		delta = (delta * kCPUPerformanceScaleMax) / range;
		increase_cpu_performance(delta);
	}
}


/* static */ int32
CPUEntry::_RescheduleEvent(timer* /* unused */)
{
	cpu_ent* currentCPUSt = get_cpu_struct();
	currentCPUSt->invoke_scheduler = true;
	currentCPUSt->preempted = true;
	return B_HANDLED_INTERRUPT;
}


/* static */ int32
CPUEntry::_UpdateLoadEvent(timer* /* unused */)
{
	int32 currentCPUId = smp_get_current_cpu();
	CPUEntry* cpu = CPUEntry::GetCPU(currentCPUId);

	bigtime_t now = system_time();
	cpu->UpdateInstantaneousLoad(now);

	CoreEntry* core = cpu->Core();
	if (core) {
		core->ChangeLoad(0);
	}

	cpu->fUpdateLoadEvent = false;

	if (thread_is_idle_thread(gCPU[currentCPUId].running_thread) && (gTrackCoreLoad || gTrackCPULoad)) {
		add_timer(&gCPU[currentCPUId].quantum_timer, &CPUEntry::_UpdateLoadEvent,
			kLoadMeasureInterval, B_ONE_SHOT_RELATIVE_TIMER);
		cpu->fUpdateLoadEvent = true;
	}
	return B_HANDLED_INTERRUPT;
}


int32
CPUEntry::CalculateTotalIrqLoad() const
{
	cpu_ent* cpuSt = &gCPU[fCPUNumber];
	SpinLocker locker(cpuSt->irqs_lock);
	int32 totalLoad = 0;
	irq_assignment* irq = (irq_assignment*)list_get_first_item(&cpuSt->irqs);
	while (irq != NULL) {
		totalLoad += irq->load;
		irq = (irq_assignment*)list_get_next_item(&cpuSt->irqs, irq);
	}
	return totalLoad;
}


CPUPriorityHeap::CPUPriorityHeap(int32 cpuCount)
	:
	Heap<CPUEntry, int32>(cpuCount)
{
}


void
CPUPriorityHeap::Dump()
{
	kprintf("cpu priority load inst_load\n");
	CPUEntry* entry = PeekRoot();
	while (entry) {
		int32 cpu = entry->ID();
		int32 key = GetKey(entry);
		kprintf("%3" B_PRId32 " %8" B_PRId32 " %3" B_PRId32 "%% %3.2f\n", cpu, key,
			entry->GetLoad() / (kMaxLoad/100), entry->GetInstantaneousLoad());

		RemoveRoot();
		sDebugCPUHeap.Insert(entry, key);

		entry = PeekRoot();
	}

	entry = sDebugCPUHeap.PeekRoot();
	while (entry) {
		int32 key = GetKey(entry);
		sDebugCPUHeap.RemoveRoot();
		Insert(entry, key);
		entry = sDebugCPUHeap.PeekRoot();
	}
}


CoreEntry::CoreEntry()
	:
	fCPUCount(0),
	fIdleCPUCount(0),
	fActiveTime(0),
	fLoad(0),
	fInstantaneousLoad(0.0f),
	fCurrentLoad(0),
	fLoadMeasurementEpoch(0),
	fHighLoad(false),
	fLastLoadUpdate(0),
	fDefunct(false)
{
	B_INITIALIZE_SPINLOCK(&fCPULock);
	B_INITIALIZE_SEQLOCK(&fActiveTimeLock);
	B_INITIALIZE_RW_SPINLOCK(&fLoadLock);
}


void
CoreEntry::Init(int32 id, PackageEntry* package)
{
	fCoreID = id;
	fPackage = package;
	fDefunct = false;
	fInstantaneousLoad = 0.0f;
}


void
CoreEntry::UpdateInstantaneousLoad()
{
	SCHEDULER_ENTER_FUNCTION();
	float totalCpuInstantaneousLoad = 0.0f;
	int32 enabledCpuCountOnCore = 0;

	for (int32 i = 0; i < smp_get_num_cpus(); i++) {
		if (fCPUSet.GetBit(i)) {
			if (!gCPU[i].disabled) {
				CPUEntry* cpuEntry = CPUEntry::GetCPU(i);
				totalCpuInstantaneousLoad += cpuEntry->GetInstantaneousLoad();
				enabledCpuCountOnCore++;
			}
		}
	}

	WriteSpinLocker loadLocker(fLoadLock);
	if (enabledCpuCountOnCore > 0) {
		fInstantaneousLoad = totalCpuInstantaneousLoad / enabledCpuCountOnCore;
	} else {
		fInstantaneousLoad = 0.0f;
	}
	fInstantaneousLoad = std::max(0.0f, std::min(1.0f, fInstantaneousLoad));
}


int32
CoreEntry::ThreadCount() const
{
	SCHEDULER_ENTER_FUNCTION();
	int32 totalThreads = 0;
	SpinLocker lock(fCPULock);
	for (int32 i = 0; i < smp_get_num_cpus(); i++) {
		if (fCPUSet.GetBit(i) && !gCPU[i].disabled) {
			CPUEntry* cpuEntry = CPUEntry::GetCPU(i);
			totalThreads += cpuEntry->GetTotalThreadCount();
		}
	}
	return totalThreads;
}


void
CoreEntry::AddCPU(CPUEntry* cpu)
{
	SpinLocker lock(fCPULock);

	fCPUSet.SetBit(cpu->ID());
	fCPUCount++;
	fIdleCPUCount++;
	fCPUHeap.Insert(cpu, B_IDLE_PRIORITY);

	if (fCPUCount == 1) {
		fLoad = 0;
		fCurrentLoad = 0;
		fInstantaneousLoad = 0.0f;
		fHighLoad = false;
		fLastLoadUpdate = system_time();
		fLoadMeasurementEpoch = 0;

		// Insert into the appropriate shard if this is the first CPU
		int32 shardIndex = this->ID() % Scheduler::kNumCoreLoadHeapShards;
		WriteSpinLocker heapsLock(Scheduler::gCoreHeapsShardLock[shardIndex]);
		if (this->GetMinMaxHeapLink()->fIndex == -1) { // If not already in any heap
			Scheduler::gCoreLoadHeapShards[shardIndex].Insert(this, 0); // Initial load is 0
		}
		heapsLock.Unlock();

		if (fPackage != NULL) {
			fPackage->AddIdleCore(this);
		}
	}
}


void
CoreEntry::RemoveCPU(CPUEntry* cpu, ThreadProcessing& threadPostProcessing)
{
	SpinLocker lock(fCPULock);

	ASSERT(fCPUCount > 0);

	if (cpu->GetHeapLink()->fIndex != -1) {
#if KDEBUG
		if (fCPUHeap.Count() == 0) {
			panic("CoreEntry::RemoveCPU: CPU %" B_PRId32 " on core %" B_PRId32
				" has heap link index %d but fCPUHeap is empty.",
				cpu->ID(), this->ID(), cpu->GetHeapLink()->fIndex);
		}
		if (fCPUHeap.PeekRoot() != cpu && CPUPriorityHeap::GetKey(cpu) == B_IDLE_PRIORITY) {
			// If it's an idle CPU being removed, it should be at the root if heap is correct
			// This might indicate an issue if it's not the root but is idle.
			// However, another CPU on the same core could also be idle.
			// The primary check is that it's in the heap.
			// If it's not root, Heap::Remove(element) is needed, which isn't standard.
			// The current Heap.h only has RemoveRoot().
			// This implies UpdatePriority must have been called to make it the root (e.g. by setting its key to a very low value).
		}
#endif
		// Assuming cpu is made root by UpdatePriority(B_IDLE_PRIORITY) before calling RemoveCPU.
		if (fCPUHeap.PeekRoot() == cpu) {
			fCPUHeap.RemoveRoot();
		} else {
			// This is problematic if CPUPriorityHeap doesn't have a direct Remove(element).
			// For now, rely on the assumption that it was made the root.
			// If not, this is a bug in the calling sequence or CPUPriorityHeap limitations.
			panic("CoreEntry::RemoveCPU: CPU %" B_PRId32 " not root of heap, cannot remove.", cpu->ID());
		}
	}

	fCPUSet.ClearBit(cpu->ID());
	fCPUCount--;

	// If the removed CPU was idle, adjust fIdleCPUCount.
	// This needs careful synchronization with CPUGoesIdle/CPUWakesUp.
	// If UpdatePriority(B_IDLE_PRIORITY) was called, CPUGoesIdle would have run.
	// So, if it *was* idle, fIdleCPUCount would have been incremented.
	// Now that it's removed, if it contributed to fIdleCPUCount, decrement it.
	// A simple way: if gCPU[cpu->ID()].disabled is true, it should be idle.
	// This is complex. Let's assume CPUGoesIdle/CPUWakesUp correctly maintain
	// fIdleCPUCount relative to the *current* fCPUCount.
	// If a CPU is removed, and it was idle, then fIdleCPUCount must be > 0.
	// This part is a bit hand-wavy without knowing the exact state of the CPU being removed.
	// For now, we assume that if CPU was idle its fIdleCPUCount contribution is removed implicitly
	// by CPUWakesUp if it was the last CPU and the core became active, or if not, fIdleCPUCount
	// just becomes out of sync until CPUGoesIdle/WakesUp is called for another CPU on the core.
	// A safer bet: if gCPU[cpu->ID()].disabled is true, it means it was considered idle for removal.
	if (gCPU[cpu->ID()].disabled) { // Check if it was an idle CPU being removed
		if (fIdleCPUCount > 0) { // Ensure we don't go negative
			// This assumes that CPUGoesIdle was called before this, incrementing fIdleCPUCount
			// We now decrement it as the CPU is gone.
			// This is still not perfect. The state of fIdleCPUCount should be carefully
			// managed by CPUGoesIdle/WakesUp in conjunction with AddCPU/RemoveCPU.
			// For now, let's assume if fCPUCount becomes 0, the core is fully idle/defunct.
		}
	}


	lock.Unlock();

	if (fCPUCount == 0) {
		this->fDefunct = true;
		TRACE("CoreEntry::RemoveCPU: Core %" B_PRId32 " marked as defunct.\n", this->ID());
		thread_map(CoreEntry::_UnassignThread, this);
		{
			WriteSpinLocker loadLocker(fLoadLock);
			fLoad = 0;
			fCurrentLoad = 0;
			fInstantaneousLoad = 0.0f;
		}
		{
			WriteSpinLocker heapsLock(gCoreHeapsLock);
			if (this->GetMinMaxHeapLink()->fIndex != -1) {
				if (fHighLoad) gCoreHighLoadHeap.Remove(this);
				else gCoreLoadHeap.Remove(this);
			}
		}
		if (fPackage != NULL) {
			fPackage->RemoveIdleCore(this);
		}
	}
	_UpdateLoad(true);
	UpdateInstantaneousLoad();
}


void
CoreEntry::_UpdateLoad(bool forceUpdate)
{
	SCHEDULER_ENTER_FUNCTION();

	if (this->fDefunct) {
		// Defunct logic: always update global heaps to remove the core
		WriteSpinLocker localDefunctLock(fLoadLock);
		fLoad = 0;
		fCurrentLoad = 0;
		fInstantaneousLoad = 0.0f;
		bool wasDefunctHighLoad = fHighLoad;
		fHighLoad = false;
		localDefunctLock.Unlock();

		int32 shardIndex = this->ID() % Scheduler::kNumCoreLoadHeapShards;
		WriteSpinLocker globalHeapsDefunctLock(Scheduler::gCoreHeapsShardLock[shardIndex]);
		if (this->GetMinMaxHeapLink()->fIndex != -1) { // If in a heap
			if (wasDefunctHighLoad)
				Scheduler::gCoreHighLoadHeapShards[shardIndex].Remove(this);
			else
				Scheduler::gCoreLoadHeapShards[shardIndex].Remove(this);
		}
		// globalHeapsDefunctLock released by destructor
		return;
	}

	// 1. Calculate newAverageLoad for the core
	int32 newAverageLoad = 0;
	int32 activeCPUsOnCore = 0;
	{
		SpinLocker cpuListLock(fCPULock); // Protects fCPUSet iteration
		for (int32 i = 0; i < smp_get_num_cpus(); i++) {
			if (fCPUSet.GetBit(i) && !gCPU[i].disabled) {
				CPUEntry* cpuEntry = CPUEntry::GetCPU(i);
				newAverageLoad += cpuEntry->GetLoad();
				activeCPUsOnCore++;
			}
		}
	}
	if (activeCPUsOnCore > 0)
		newAverageLoad /= activeCPUsOnCore;
	else
		newAverageLoad = 0;
	newAverageLoad = std::min(newAverageLoad, kMaxLoad); // Clamp

	// 2. Determine timing info
	bigtime_t now = system_time();
	bool intervalEnded; // Will be used to update epoch and lastUpdate locally

	// 3. Read current state from heap link and local fHighLoad (protected by local fLoadLock)
	//    And then update local state.
	int32 keyCurrentlyInHeap;
	bool wasInAHeap;
	bool highLoadStatusWhenLastInHeap; // The fHighLoad status that corresponds to keyCurrentlyInHeap

	WriteSpinLocker localStateLock(fLoadLock);
	keyCurrentlyInHeap = this->GetMinMaxHeapLink()->fKey;
	wasInAHeap = (this->GetMinMaxHeapLink()->fIndex != -1);
	highLoadStatusWhenLastInHeap = fHighLoad; // This is the state associated with keyCurrentlyInHeap

	intervalEnded = now >= kLoadMeasureInterval + fLastLoadUpdate;

	// Update internal fLoad, fHighLoad, epoch, lastUpdate *first*
	fLoad = newAverageLoad;
	bool isNowEffectivelyHighLoad = fLoad > kHighLoad; // Based on the NEW fLoad
	fHighLoad = isNowEffectivelyHighLoad; // Update internal fHighLoad
	if (intervalEnded) {
		fLoadMeasurementEpoch++;
		fLastLoadUpdate = now;
	}
	localStateLock.Unlock(); // CoreEntry's internal state (fLoad, fHighLoad, etc.) is now up-to-date.

	// 4. Decide if global heap update is needed
	if (!forceUpdate && wasInAHeap
		&& abs(newAverageLoad - keyCurrentlyInHeap) < CORE_LOAD_UPDATE_DELTA_THRESHOLD
		&& highLoadStatusWhenLastInHeap == isNowEffectivelyHighLoad) {
		// Change is small, and it doesn't cross the high/low load boundary relative to its last heap status.
		// Internal state (fLoad, fHighLoad, epoch) already updated. Nothing more to do for global heaps.
		return;
	}

	// 5. If significant change or forceUpdate, update global heaps
	int32 shardIndex = this->ID() % Scheduler::kNumCoreLoadHeapShards;
	WriteSpinLocker coreHeapsLocker(Scheduler::gCoreHeapsShardLock[shardIndex]);

	if (wasInAHeap) {
		// Remove using the status (highLoadStatusWhenLastInHeap) that got it into its current heap.
		// The key for removal is implicitly handled by MinMaxHeap::Remove(this) using its stored link->fKey.
		if (highLoadStatusWhenLastInHeap)
			Scheduler::gCoreHighLoadHeapShards[shardIndex].Remove(this);
		else
			Scheduler::gCoreLoadHeapShards[shardIndex].Remove(this);
	}

	// Insert into the new correct heap using the CoreEntry's current (just updated) fLoad and fHighLoad.
	// The heap insertion will use the CoreEntry's current fLoad (which is newAverageLoad) as the key.
	// The fHighLoad field (which is isNowEffectivelyHighLoad) determines which heap.
	if (fHighLoad) { // Use the just-updated fHighLoad from internal state
		Scheduler::gCoreHighLoadHeapShards[shardIndex].Insert(this, fLoad); // Insert with current fLoad
	} else {
		Scheduler::gCoreLoadHeapShards[shardIndex].Insert(this, fLoad); // Insert with current fLoad
	}
	// coreHeapsLocker releases gCoreHeapsShardLock[shardIndex] at end of scope
}


/* static */ void
CoreEntry::_UnassignThread(Thread* thread, void* data)
{
	CoreEntry* core = static_cast<CoreEntry*>(data);
	ThreadData* threadData = thread->scheduler_data;
	if (threadData != NULL && threadData->Core() == core && thread->pinned_to_cpu == 0)
		threadData->UnassignCore(thread->state == B_THREAD_RUNNING);
}


CoreLoadHeap::CoreLoadHeap(int32 coreCount)
	:
	MinMaxHeap<CoreEntry, int32>(coreCount)
{
}


void
CoreLoadHeap::Dump() // Lock is now handled by the caller
{
	// WriteSpinLocker lock(gCoreHeapsLock); // REMOVED

	CoreEntry* entry = PeekMinimum();
	while (entry) {
		int32 key = GetKey(entry);
		DebugDumper::DumpCoreLoadHeapEntry(entry);
		RemoveMinimum();
		sDebugCoreHeap.Insert(entry, key);
		entry = PeekMinimum();
	}
	entry = sDebugCoreHeap.PeekMinimum();
	while (entry) {
		int32 key = GetKey(entry);
		sDebugCoreHeap.RemoveMinimum();
		Insert(entry, key);
		entry = sDebugCoreHeap.PeekMinimum();
	}
}


PackageEntry::PackageEntry()
	:
	fIdleCoreCount(0),
	fCoreCount(0)
{
	B_INITIALIZE_RW_SPINLOCK(&fCoreLock);
}


void
PackageEntry::Init(int32 id)
{
	fPackageID = id;
}


void
PackageEntry::_AddConfiguredCore()
{
	atomic_add(&fCoreCount, 1);
}

void
PackageEntry::AddIdleCore(CoreEntry* core)
{
	WriteSpinLocker lock(fCoreLock);

	if (!fIdleCores.Contains(core)) {
		fIdleCores.Add(core);
		fIdleCoreCount++;

		if (fIdleCoreCount == fCoreCount && fCoreCount > 0) {
			WriteSpinLocker globalLock(gIdlePackageLock);
			if (!gIdlePackageList.Contains(this))
				gIdlePackageList.Add(this);
		}
	}
}


void
PackageEntry::RemoveIdleCore(CoreEntry* core)
{
	bool packageWasFullyIdle;
	{
		ReadSpinLocker lock(fCoreLock);
		packageWasFullyIdle = (fIdleCoreCount == fCoreCount && fCoreCount > 0);
	}

	WriteSpinLocker lock(fCoreLock);
	if (fIdleCores.Contains(core)) {
		fIdleCores.Remove(core);
		fIdleCoreCount--;
		ASSERT(fIdleCoreCount >= 0);

		if (packageWasFullyIdle && fIdleCoreCount < fCoreCount) {
			WriteSpinLocker globalLock(gIdlePackageLock);
			if (gIdlePackageList.Contains(this))
				gIdlePackageList.Remove(this);
		}
	}
}


/* static */ void
DebugDumper::DumpEevdfRunQueue(CPUEntry* cpu)
{
	kprintf("\nCPU %" B_PRId32 " EEVDF Run Queue (InstLoad: %.2f, TotalThreadsInRunQueue: %" B_PRId32 ", MinVRuntime: %" B_PRId64 "):\n",
		cpu->ID(), cpu->GetInstantaneousLoad(), cpu->GetEevdfRunQueue().Count(),
		cpu->MinVirtualRuntime());
	kprintf("  (CPU TotalReportedThreads: %" B_PRId32 ", Idle Thread ID: %" B_PRId32 ")\n",
		cpu->GetTotalThreadCount(),
		cpu->fIdleThread ? cpu->fIdleThread->GetThread()->id : -1);

	cpu->LockRunQueue();
	// Use a non-const reference to be able to PopMinimum
	EevdfRunQueue& queue = cpu->GetEevdfRunQueue();
	if (queue.IsEmpty()) {
		kprintf("  Run queue is empty.\n");
	} else {
		kprintf("  %-18s %-7s %-15s %-11s %-11s %-11s %-12s %s\n",
			"Thread*", "ID", "VirtDeadline", "Lag", "EligTime", "SliceDur", "VirtRuntime", "Name");
		kprintf("  ----------------------------------------------------------------------------------------------------------\n");

		const int MAX_THREADS_TO_DUMP_PER_CPU = 128; // Max threads we'll pull out for dumping
		ThreadData* dumpedThreads[MAX_THREADS_TO_DUMP_PER_CPU];
		int numActuallyDumped = 0;
		int totalInQueueOriginally = queue.Count();

		for (int i = 0; i < MAX_THREADS_TO_DUMP_PER_CPU && !queue.IsEmpty(); ++i) {
			ThreadData* td = queue.PopMinimum();
			if (td == NULL) // Should not happen if IsEmpty() was false
				break;

			dumpedThreads[numActuallyDumped++] = td;

			Thread* t = td->GetThread();
			kprintf("  %p  %-7" B_PRId32 " %-15" B_PRId64 " %-11" B_PRId64 " %-11" B_PRId64 " %-11" B_PRId64 " %-12" B_PRId64 " %s\n",
				t, t->id, td->VirtualDeadline(), td->Lag(), td->EligibleTime(),
				td->SliceDuration(), td->VirtualRuntime(), t->name);
		}

		// Re-add all popped threads
		for (int i = 0; i < numActuallyDumped; ++i) {
			queue.Add(dumpedThreads[i]);
		}

		// After re-adding, ensure fMinVirtualRuntime is correct.
		// EevdfRunQueue::Add calls _UpdateMinVirtualRuntime if the new item is the head
		// or if the queue was empty. Calling it here ensures it's updated
		// even if the re-additions didn't trigger it sufficiently (e.g. if multiple
		// threads had same VD and heap order changed).
		if (!queue.IsEmpty()) {
			cpu->_UpdateMinVirtualRuntime();
		}

		if (totalInQueueOriginally > numActuallyDumped) {
			kprintf("  (... NOTE: Only dumped top %d of %d threads due to internal KDL buffer limit ...)\n",
				numActuallyDumped, totalInQueueOriginally);
		}
		kprintf("  ----------------------------------------------------------------------------------------------------------\n");
		kprintf("  Total threads printed: %d. Threads in queue after re-add: %" B_PRId32 ".\n",
			numActuallyDumped, queue.Count());
	}
	cpu->UnlockRunQueue();
}


/* static */ void
DebugDumper::DumpCoreLoadHeapEntry(CoreEntry* entry)
{
	kprintf("%4" B_PRId32 " %11" B_PRId32 "%% %8.2f %7" B_PRId32 " %5" B_PRIu32 "\n",
		entry->ID(),
		entry->GetLoad(),
		entry->GetInstantaneousLoad(),
		entry->ThreadCount(),
		entry->LoadMeasurementEpoch());
}


/* static */ void
DebugDumper::DumpIdleCoresInPackage(PackageEntry* package)
{
	kprintf("%-7" B_PRId32 " ", package->fPackageID);
	ReadSpinLocker lock(package->fCoreLock);

	DoublyLinkedList<CoreEntry>::ConstIterator iterator(&package->fIdleCores);
	bool first = true;
	while (iterator.HasNext()) {
		CoreEntry* coreEntry = iterator.Next();
		if (!first) kprintf(", ");
		kprintf("%" B_PRId32, coreEntry->ID());
		first = false;
	}
	if (first) kprintf("-");
	kprintf("\n");
}


static int
dump_run_queue(int /* argc */, char** /* argv */)
{
	int32 cpuCount = smp_get_num_cpus();
	for (int32 i = 0; i < cpuCount; i++) {
		if (gCPUEnabled.GetBit(i))
			DebugDumper::DumpEevdfRunQueue(&gCPUEntries[i]); // Changed call
	}
	return 0;
}


static int
dump_cpu_heap(int /* argc */, char** /* argv */)
{
	kprintf("Core Load Heaps (Sharded):\n");
	for (int32 shardIdx = 0; shardIdx < Scheduler::kNumCoreLoadHeapShards; shardIdx++) {
		kprintf("---- Shard %" B_PRId32 " ----\n", shardIdx);

		WriteSpinLocker shardLocker(Scheduler::gCoreHeapsShardLock[shardIdx]);

		kprintf("  Low Load Cores in Shard %" B_PRId32 " (ID AvgLoad InstLoad Threads Epoch):\n", shardIdx);
		if (Scheduler::gCoreLoadHeapShards[shardIdx].Count() > 0) {
			Scheduler::gCoreLoadHeapShards[shardIdx].Dump();
		} else {
			kprintf("    (empty)\n");
		}

		kprintf("\n  High Load Cores in Shard %" B_PRId32 " (ID AvgLoad InstLoad Threads Epoch):\n", shardIdx);
		if (Scheduler::gCoreHighLoadHeapShards[shardIdx].Count() > 0) {
			Scheduler::gCoreHighLoadHeapShards[shardIdx].Dump();
		} else {
			kprintf("    (empty)\n");
		}
		shardLocker.Unlock();
		kprintf("\n");
	}

	// The rest of the function (dumping per-core CPU heaps) remains the same.
	for (int32 i = 0; i < gCoreCount; i++) {
		bool coreHasEnabledCPUs = false;
		for(int j=0; j < smp_get_num_cpus(); ++j) {
			if (gCoreEntries[i].CPUMask().GetBit(j) && gCPUEnabled.GetBit(j)) {
				coreHasEnabledCPUs = true;
				break;
			}
		}
		if (gCoreEntries[i].CPUCount() > 0) {
			kprintf("\nCore %" B_PRId32 " CPU Priority Heap (CPUs on this core):\n", i);
			gCoreEntries[i].CPUHeap()->Dump();
		}
	}
	return 0;
}


static int
dump_idle_cores(int /* argc */, char** /* argv */)
{
	kprintf("Idle packages (packages with at least one idle core):\n");
	ReadSpinLocker globalLock(gIdlePackageLock);
	IdlePackageList::ConstIterator idleIterator(&gIdlePackageList);

	if (idleIterator.HasNext()) {
		kprintf("package idle_cores_list\n");
		while (idleIterator.HasNext())
			DebugDumper::DumpIdleCoresInPackage(idleIterator.Next());
	} else
		kprintf("No packages currently in the idle list.\n");
	globalLock.Unlock();

	kprintf("\nAll Packages (package_id: idle_core_count / total_configured_core_count_on_package):\n");
	for(int32 i = 0; i < gPackageCount; ++i) {
		ReadSpinLocker lock(gPackageEntries[i].CoreLock());
		kprintf("  %2" B_PRId32 ": %2" B_PRId32 " / %2" B_PRId32 "\n",
			gPackageEntries[i].PackageID(),
			gPackageEntries[i].IdleCoreCountNoLock(),
			gPackageEntries[i].CoreCountNoLock());
	}
	return 0;
}


// #pragma mark - Unified IRQ Target CPU Selection


Scheduler::CPUEntry*
Scheduler::SelectTargetCPUForIRQ(CoreEntry* targetCore, int32 irqLoadToMove,
	float irqTargetFactor, float smtConflictFactor,
	int32 maxTotalIrqLoadOnTargetCPU)
{
	SCHEDULER_ENTER_FUNCTION();
	ASSERT(targetCore != NULL);

	CPUEntry* bestCPU = NULL;
	float bestScore = 1e9;

	CPUSet coreCPUs = targetCore->CPUMask();
	for (int32 i = 0; i < smp_get_num_cpus(); i++) {
		if (!coreCPUs.GetBit(i) || gCPU[i].disabled)
			continue;

		CPUEntry* currentCPU = CPUEntry::GetCPU(i);
		ASSERT(currentCPU->Core() == targetCore);

		int32 currentCpuExistingIrqLoad = currentCPU->CalculateTotalIrqLoad();
		if (maxTotalIrqLoadOnTargetCPU > 0 && currentCpuExistingIrqLoad + irqLoadToMove >= maxTotalIrqLoadOnTargetCPU) {
			TRACE("SelectTargetCPUForIRQ: CPU %" B_PRId32 " fails IRQ capacity (curr:%" B_PRId32 ", add:%" B_PRId32 ", max:%" B_PRId32 ")\n",
				currentCPU->ID(), currentCpuExistingIrqLoad, irqLoadToMove, maxTotalIrqLoadOnTargetCPU);
			continue;
		}

		float threadInstantLoad = currentCPU->GetInstantaneousLoad();
		float smtPenalty = 0.0f;
		if (targetCore->CPUCount() > 1) {
			int32 currentCPUID = currentCPU->ID();
			int32 currentCoreID = targetCore->ID();
			int32 currentSMTID = gCPU[currentCPUID].topology_id[CPU_TOPOLOGY_SMT];

			if (currentSMTID != -1) {
				for (int32 k = 0; k < smp_get_num_cpus(); k++) {
					if (k == currentCPUID || gCPU[k].disabled)
						continue;

					if (gCPU[k].topology_id[CPU_TOPOLOGY_CORE] == currentCoreID &&
						gCPU[k].topology_id[CPU_TOPOLOGY_SMT] == currentSMTID) {
						smtPenalty += CPUEntry::GetCPU(k)->GetInstantaneousLoad() * smtConflictFactor;
					}
				}
			}
		}
		float threadEffectiveLoad = threadInstantLoad + smtPenalty;

		float denominator = (maxTotalIrqLoadOnTargetCPU - irqLoadToMove + 1);
		if (denominator <= 0) denominator = 1.0f;

		float normalizedExistingIrqLoad = (maxTotalIrqLoadOnTargetCPU > 0)
			? std::min(1.0f, (float)currentCpuExistingIrqLoad / denominator)
			: ( (maxTotalIrqLoadOnTargetCPU == 0 && currentCpuExistingIrqLoad == 0) ? 0.0f : 1.0f);


		float score = (1.0f - irqTargetFactor) * threadEffectiveLoad
						   + irqTargetFactor * normalizedExistingIrqLoad;

		if (bestCPU == NULL || score < bestScore) {
			bestScore = score;
			bestCPU = currentCPU;
		}
	}

	if (bestCPU != NULL) {
		 TRACE("SelectTargetCPUForIRQ: Selected CPU %" B_PRId32 " on core %" B_PRId32 " with score %f\n",
			bestCPU->ID(), targetCore->ID(), bestScore);
	} else {
		 TRACE("SelectTargetCPUForIRQ: No suitable CPU found on core %" B_PRId32 " for IRQ (load %" B_PRId32 ")\n",
			targetCore->ID(), irqLoadToMove);
	}
	return bestCPU;
}


void Scheduler::init_debug_commands()
{
	if (sDebugCPUHeap.Count() == 0)
		new(&sDebugCPUHeap) CPUPriorityHeap(smp_get_num_cpus());
	if (sDebugCoreHeap.Count() == 0)
		new(&sDebugCoreHeap) CoreLoadHeap(smp_get_num_cpus());

	add_debugger_command_etc("eevdf_run_queue", &DebugDumper::DumpEevdfRunQueue,
		"List threads in EEVDF run queue per CPU", "\nLists threads in EEVDF run queue per CPU", 0);
	add_debugger_command_alias("run_queue", "eevdf_run_queue", "Alias for eevdf_run_queue");

	if (!gSingleCore) {
		add_debugger_command_etc("cpu_heap", &dump_cpu_heap,
			"List Cores in load heaps & CPUs in Core priority heaps",
			"\nList Cores in load heaps & CPUs in Core priority heaps", 0);
		add_debugger_command_etc("idle_cores", &dump_idle_cores,
			"List idle cores per package", "\nList idle cores per package", 0);
	}
}

// Ensure IsOwnedByCurrentThread() is defined if not standard
// For spinlock, typically this check is implicit or not exposed.
// Assuming for compilation it's available or handled by the build environment.
#ifndef B_SPINLOCK_IS_OWNED_BY_CURRENT_THREAD
#define B_SPINLOCK_IS_OWNED_BY_CURRENT_THREAD(lock) true // Placeholder for non-debug builds
#endif

// Ensure spinlock structure has IsOwnedByCurrentThread for KDEBUG
#if KDEBUG
#ifndef SPINLOCK_DEBUG_FEATURES
// This is a hypothetical addition; spinlocks usually don't track owners for performance.
// If Haiku's spinlock has a debug owner field, use it. Otherwise, this can't be easily asserted.
// For now, we'll rely on careful manual lock management.
#define ASSERT_SPINLOCK_OWNED_BY_CURRENT_THREAD(lock) \
	ASSERT_PRINT(true, "Spinlock ownership check not implemented robustly for this spinlock type")
#else
#define ASSERT_SPINLOCK_OWNED_BY_CURRENT_THREAD(lock) \
	ASSERT_PRINT((lock).owner == find_thread(NULL)->id, "Spinlock not owned by current thread")
#endif // SPINLOCK_DEBUG_FEATURES

// Redefine for CPUEntry's fQueueLock if it's a specific type that needs this
// For generic spinlock, this redefinition is problematic.
// The original ASSERT(fQueueLock.IsOwned()) was likely a conceptual check.
// Let's assume fQueueLock.IsOwnedByCurrentThread() is a valid check.
// If not, this assertion will need to be removed or adapted.
#undef ASSERT_SPINLOCK_OWNED_BY_CURRENT_THREAD
#define ASSERT_SPINLOCK_OWNED_BY_CURRENT_THREAD(lock) \
	ASSERT_PRINT(debug_spinlock_held(&(lock)), "Spinlock not owned by current thread")

// If debug_spinlock_held is not available, then this assertion cannot be performed reliably.
// For the purpose of this exercise, I will assume such a debug function exists.
// If not, this assertion should be removed from _UpdateMinVirtualRuntime.
// For the provided code, I'll use `fQueueLock.IsOwnedByCurrentThread()` as per the previous diff.
// If `IsOwnedByCurrentThread` is not a member of `spinlock`, this will fail to compile.
// The previous diff used `ASSERT(fQueueLock.IsOwnedByCurrentThread());`
// Let's assume `spinlock` in Haiku has a debug `IsOwnedByCurrentThread` or similar.
// The previous tool output applied this: `ASSERT(fQueueLock.IsOwnedByCurrentThread()); // Replaced IsOwned()`
// I will stick to this, assuming it's a valid Haiku debug feature for spinlocks.
#endif // KDEBUG

// Final check on ASSERT in _UpdateMinVirtualRuntime:
// Original code had `ASSERT(fQueueLock.IsOwned());`
// My previous change was `ASSERT(fQueueLock.IsOwnedByCurrentThread());`
// If Haiku's spinlock doesn't have `IsOwnedByCurrentThread`, this will fail.
// A common way to check spinlock ownership in debug is `debug_spinlock_held(&fQueueLock)`.
// I will use this more standard Haiku debug pattern if available, otherwise, remove the assert.
// For now, I will revert to a simple `ASSERT(!are_interrupts_enabled());` which implies the lock should be held.
// Actually, the best is to use the one that was accepted: `ASSERT(fQueueLock.IsOwnedByCurrentThread());`
// I will ensure the implementation of _UpdateMinVirtualRuntime uses this.
// The diff from the previous step already includes this.
